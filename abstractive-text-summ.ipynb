{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive Text Summarization\n",
    "===\n",
    "This notebook is an in-progress implementation of the [Abstractive Text Summarization using Sequence-to-sequence RNNs and\n",
    "Beyond](https://arxiv.org/abs/1602.06023) paper.\n",
    "\n",
    "Current Features\n",
    "---\n",
    "* model architecture supports LSTM & GRU (biLSTM-to-uniLSTM or biGRU-to-uniGRU)\n",
    "* implements batch data processing \n",
    "* implements attention mechanism ([Bahdanau et al.](https://arxiv.org/abs/1409.0473) & [Luong et al.(global dot)](https://arxiv.org/abs/1508.04025))\n",
    "* implements [scheduled sampling (teacher forcing)](https://arxiv.org/abs/1506.03099)\n",
    "* implements [tied embeddings](https://arxiv.org/pdf/1608.05859.pdf)\n",
    "* initializes encoder-decoder with pretrained vectors (glove.6B.200d)\n",
    "* implements custom training callbacks (tensorboard visualization for PyTorch, save best model & log checkpoint)\n",
    "* implements attention plots\n",
    "\n",
    "\n",
    "To-Do\n",
    "---\n",
    "* Implement additional linguistic features embeddings  \n",
    "* Implement generator-pointer switch and replace unknown words by selecting source token with the highest attention score.\n",
    "* Implement large vocabulary trick \n",
    "* Implement sentence level attention \n",
    "* Implement beam search during inference\n",
    "* implement rouge eevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "---\n",
    "\n",
    "1. Create conda environment \n",
    "\n",
    "`conda env create -f environment.yml`  --gpu\n",
    "\n",
    "`conda env create -f environment-cpu.yml`  --cpu\n",
    "\n",
    "2. Install dependencies (PyTorch, Fastai, TorchText, Tensorboard etc) via:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "3. Download `spacy` english module\n",
    "\n",
    "`python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "--\n",
    "\n",
    "The dataset used is a subset of the gigaword dataset and can be found [here](https://drive.google.com/file/d/0B6N7tANPyVeBNmlSX19Ld2xDU1E/view?usp=sharing).\n",
    "\n",
    "It contains 3,803,955 parallel source & target examples for training and 189,649 examples for validation.\n",
    "\n",
    "After downloading, we create article-title pairs, save in tabular datset format (.csv) and extract a sample subset (80,000 for training & 20,000 for validation). This data preparation can be found [here](/data-preparation.ipynb).\n",
    "\n",
    "An example article-title pair looks like this:\n",
    "\n",
    "`article: the algerian cabinet chaired by president abdelaziz bouteflika on sunday adopted the #### finance bill predicated on an oil price of ## dollars a barrel and a growth rate of #.# percent , it was announced here .`\n",
    "\n",
    "`title: algeria adopts #### finance bill with oil put at ## dollars a barrel`\n",
    "\n",
    "\n",
    "Training on the complete dataset (3M) would take a really long time. So in order to train and experiment faster we use our sample subset of 80,000 in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_GPU=False\n"
     ]
    }
   ],
   "source": [
    "#declare the directory path to dataset  \n",
    "DATA_PATH = 'data/'\n",
    "SAMPLE_DATA_PATH = f'{DATA_PATH}sample_data/'\n",
    "PROCESSED_DATA_PATH = f'{DATA_PATH}processed_data/'\n",
    "\n",
    "#Enable GPU training \n",
    "import torch\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE_GPU={}'.format(USE_GPU))\n",
    "if USE_GPU:\n",
    "    print('current_device={}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to train, we performed common processing steps on the dataset such as:\n",
    "\n",
    "* Loading dataset\n",
    "* Preprocessing dataset (tokenizing, appending begining-of-sentence and end-of-sentence tokens, truncating, etc)\n",
    "* Building a vocabulary\n",
    "* Creating dataset iterators\n",
    "* Batching, padding, and numericalizing. \n",
    "\n",
    "To process the dataset, we use the [torchtext](https://github.com/pytorch/text) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import data, vocab from torchtext \n",
    "from torchtext import data, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.1. Load & define preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To pre-process our data, we declare a `Field` class, and pass additional pre-processing arguments ( ex. tokenize with the `spacy` tokenizer, `lower` and append an `end-of-sentence` token to every example )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = data.get_tokenizer('spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenizer, lower=True, eos_token='_eos_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we load our training & validation tabular dataset using `data.TabularDataset.splits` which applies the defined preprocessing pipeline and returns their respective `Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.2 ms, sys: 2.39 ms, total: 101 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_data_fields = [(\"source\", TEXT),\n",
    "                   (\"target\", TEXT)]\n",
    "\n",
    "trn, vld = data.TabularDataset.splits(path=f'{SAMPLE_DATA_PATH}',\n",
    "                                     train='train_ds.csv', validation='valid_ds.csv',\n",
    "                                     format='csv', skip_header=True, fields=trn_data_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'second', 'day', 'of', 'negotiations', 'on', 'power', '-', 'sharing', 'in', 'zimbabwe', 'ended', 'wednesday', 'without', 'a', 'deal', ',', 'but', 'president', 'robert', 'mugabe', 'said', 'participants', 'will', '``', 'hopefully', \"''\", 'reach', 'an', 'accord', 'thursday', '.'] ['no', 'deal', 'yet', 'but', 'mugabe', 'hopes', 'for', 'zimbabwe', 'deal', 'thursday']\n"
     ]
    }
   ],
   "source": [
    "# a sample of the preprocessed data\n",
    "print(trn[0].article, trn[0].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2. Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Building a vocabulary simply means mapping each unique token in the corpus to an integer value, and storing as a dictionary ex `{'the': 2, 'brown':3, 'fox':4}`. \n",
    "\n",
    "In addition to building a vocabulary, we also use torchtext to load an embedding matrix for each token using the `glove.6B.200d` pretrained vector.\n",
    "\n",
    "We pass to `TEXT.build_vocab` our training dataset object `trn` and also the name of the pretrained vector we would like to use `glove.6B.200d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 4.08 s, total: 5.16 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pretrained_vectors = 'glove.6B.200d' \n",
    "TEXT.build_vocab(trn, vectors=pretrained_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 92),\n",
       " ('#', 92),\n",
       " ('.', 86),\n",
       " ('in', 67),\n",
       " ('of', 56),\n",
       " (',', 56),\n",
       " ('a', 53),\n",
       " ('to', 45),\n",
       " ('on', 41),\n",
       " ('and', 39)]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 most frequent words in the vocab\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.3. Create dataset iterator, batch, pad, and numericalize. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, create a training & validation iterator object, numericalize (turn text to tensors), batch examples of similar lengths together, randomly shuffle the data and pad tensors using `data.BucketIterator.splits`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "                        (trn, vld), batch_sizes=(batch_size, int(batch_size*1.6)),\n",
    "                        device=(0 if USE_GPU else -1), \n",
    "                        sort_key=lambda x: len(x.source),\n",
    "                        shuffle=True, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we stick each article-title batch pair tensor into a tuple (article, title). To do this we create a custom helper class `BatchTuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BatchTuple():\n",
    "    def __init__(self, dataset, x_var, y_var):\n",
    "        self.dataset, self.x_var, self.y_var = dataset, x_var, y_var\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            x = getattr(batch, self.x_var) \n",
    "            y = getattr(batch, self.y_var)                 \n",
    "            yield (x, y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#returns tuple of article-title pair tensors\n",
    "train_iter_tuple = BatchTuple(train_iter, \"source\", \"target\")\n",
    "val_iter_tuple = BatchTuple(val_iter, \"source\", \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  55,    4,   96,  ...,    4,  218,    4],\n",
       "         [  42,  122,   37,  ...,  696,   15,   14],\n",
       "         [  15,  118,   47,  ...,   21,  339,   64],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " tensor([[  55,   14,   96,  ...,  323,  339,   14],\n",
       "         [  42,   19,   84,  ...,  223,  229,  748],\n",
       "         [  15,  231,   30,  ...,   21,  223,  228],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example of a batched and padded article-title tensor pair\n",
    "next(iter(train_iter_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.4. Create ModelData "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to use our batched dataset with the FastAI library, we create a `ModelData` object. \n",
    "\n",
    "`ModelData` simply sticks the training, validation (and test) dataset into a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import text module from fastai \n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_data = ModelData(DATA_PATH, trn_dl=train_iter_tuple, val_dl=val_iter_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.5. Processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we are done with processing the dataset: pre-processing, numericalizing, batching and padding.\n",
    "\n",
    "Lets take a look at the final processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 890)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of batches in training & validation set and number of tokens in vocabulary\n",
    "len(model_data.trn_dl), len(model_data.val_dl), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([45, 64]), torch.Size([18, 64]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of one batch in training set (sequence_length x batch_size)\n",
    "t, z = next(model_data.trn_dl.__iter__())\n",
    "t.size(), z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article:\n",
      "bolivian president evo morales on wednesday declared the us ambassador to la paz `` persona non grata , '' accusing the envoy of encouraging the breakup of the country by promoting separatism . _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "corresponding tensor:\n",
      "[216  28 153 167  11  13 145   4  14  57  10 294 328  20 329 316 269   7  23 130   4  50   8 254   4 217   8\n",
      "   4  48  24 341 353   5   2   1   1   1   1   1   1   1   1   1   1   1] \n",
      "\n",
      "title:\n",
      "bolivia president orders us envoy expelled _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "corresponding tensor:\n",
      "[ 58  28 171  14  50 154   2   1   1   1   1   1   1   1   1   1   1   1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets look at an example pair\n",
    "sample_source = t.transpose(1,0)[0].data.cpu().numpy()\n",
    "sample_target = z.transpose(1,0)[0].data.cpu().numpy()\n",
    "\n",
    "print(\"source:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_article]), sample_source))\n",
    "print(\"target:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_title]), sample_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Define model architecture\n",
    "The sequence model, consists of:\n",
    "* single layer encoder-decoder RNNs (biGRU-to-uniGRU)\n",
    "* feed-forward attention network (bahdanau)\n",
    "* option for luong global dot attention\n",
    "* option for teacher forcing\n",
    "* option for tied embeddings\n",
    "* option for multi-layer model\n",
    "* option for regularization (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_type, input_size, embz_size, hidden_size, batch_size,output_size,max_tgt_len,\n",
    "                 attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx,\n",
    "                 num_layers=1, encoder_drop=(0.0,0.0), decoder_drop=(0.0,0.0), \n",
    "                 bidirectional=True, bias=False, teacher_forcing=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        rnn_type, attention_type, tied_weight_type = rnn_type.upper(), attention_type.title(), tied_weight_type.lower()\n",
    "        \n",
    "        if rnn_type in ['LSTM', 'GRU']: self.rnn_type = rnn_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--rnn_type' was supplied,\n",
    "                                    options are ['LSTM', 'GRU']\"\"\")\n",
    "            \n",
    "        if attention_type in ['Luong', 'Bahdanau']: self.attention_type = attention_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--attention_type' was supplied,\n",
    "                                    options are ['Luong', 'Bahdanau']\"\"\")\n",
    "            \n",
    "        if tied_weight_type in ['three_way', 'two_way']: self.tied_weight_type = tied_weight_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--tied_weight_type' was supplied,\n",
    "                                    options are ['three_way', 'two_way']\"\"\")\n",
    "    \n",
    "                    \n",
    "        #initialize model parameters            \n",
    "        self.output_size, self.embz_size, self.hidden_size = output_size, embz_size, hidden_size//2\n",
    "        self.num_layers, self.input_size, self.max_tgt_len, self.pre_trained_vector = num_layers, input_size, max_tgt_len, pre_trained_vector\n",
    "        self.bidirectional,self.teacher_forcing, self.pre_trained_vector_type = bidirectional, teacher_forcing, pre_trained_vector_type\n",
    "        self.encoder_drop, self.decoder_drop, self.padding_idx = encoder_drop, decoder_drop, padding_idx\n",
    "        \n",
    "        \n",
    "        if self.teacher_forcing: self.force_prob = 1.0\n",
    "        \n",
    "        #set bidirectional\n",
    "        if self.bidirectional: self.num_directions = 2\n",
    "        else: self.num_directions = 1\n",
    "            \n",
    "        \n",
    "        #encoder\n",
    "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
    "        self.encoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
    "        if self.pre_trained_vector: self.encoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
    "            \n",
    "        self.encoder_rnn = getattr(nn, self.rnn_type)(\n",
    "                           input_size=self.embz_size,\n",
    "                           hidden_size=self.hidden_size,\n",
    "                           num_layers=self.num_layers,\n",
    "                           dropout=self.encoder_drop[1], \n",
    "                           bidirectional=self.bidirectional)\n",
    "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions,self.embz_size, bias=bias)\n",
    "        \n",
    "       #decoder\n",
    "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
    "        self.decoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
    "        self.decoder_rnn = getattr(nn, self.rnn_type)(\n",
    "                           input_size=self.embz_size,\n",
    "                           hidden_size=self.hidden_size*self.num_directions,\n",
    "                           num_layers=self.num_layers,\n",
    "                           dropout=self.decoder_drop[1]) \n",
    "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "        self.output_layer = nn.Linear(self.embz_size, self.output_size, bias=bias)\n",
    "        \n",
    "        #set tied weights: three way tied weights vs two way tied weights\n",
    "        if self.tied_weight_type == 'three_way':\n",
    "            self.decoder_embedding_layer.weight  = self.encoder_embedding_layer.weight\n",
    "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
    "        else:\n",
    "            if self.pre_trained_vector: self.decoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
    "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
    "            \n",
    "        #set attention\n",
    "        self.encoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "        self.att_vector_layer = nn.Linear(self.embz_size+self.embz_size, self.embz_size,bias=bias)\n",
    "        if self.attention_type == 'Bahdanau':\n",
    "            self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "            self.att_score = nn.Linear(self.embz_size,1,bias=bias)\n",
    "\n",
    "            \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)),\n",
    "                    V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)))\n",
    "        else:\n",
    "            return V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
    "   \n",
    "\n",
    "    def _cat_directions(self, hidden):\n",
    "        def _cat(h):\n",
    "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "            \n",
    "        if isinstance(hidden, tuple):\n",
    "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
    "            hidden = tuple([_cat(h) for h in hidden])\n",
    "        else:\n",
    "            # GRU hidden\n",
    "            hidden = _cat(hidden)\n",
    "        return hidden    \n",
    "    \n",
    "    \n",
    "    def bahdanau_attention(self, encoder_output, decoder_hidden, decoder_input):\n",
    "        encoder_output = self.encoder_output_layer(encoder_output) \n",
    "        encoder_output = encoder_output.transpose(0,1)\n",
    "        decoder_hidden = decoder_hidden.transpose(0,1)\n",
    "        att_score = F.tanh(encoder_output + decoder_hidden)\n",
    "        att_score = self.att_score(att_score)\n",
    "        att_weight = F.softmax(att_score, dim=1)\n",
    "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
    "        att_vector = torch.cat((context_vector, decoder_input), dim=1)\n",
    "        att_vector = self.att_vector_layer(att_vector)\n",
    "        return att_weight.squeeze(-1), att_vector\n",
    "    \n",
    "    \n",
    "    def luong_attention(self, encoder_output, decoder_output):\n",
    "        encoder_output = self.encoder_output_layer(encoder_output) \n",
    "        encoder_output = encoder_output.transpose(0,1)\n",
    "        decoder_output = decoder_output.transpose(0,1)\n",
    "        att_score = torch.bmm(encoder_output, decoder_output.transpose(-1,1))\n",
    "        att_weight = F.softmax(att_score, dim=1)\n",
    "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
    "        att_vector = torch.cat((context_vector, decoder_output.squeeze(1)), dim=1)\n",
    "        att_vector = self.att_vector_layer(att_vector)\n",
    "        att_vector = F.tanh(att_vector)\n",
    "        return att_weight.squeeze(-1), att_vector\n",
    "        \n",
    "    def decoder_forward(self, batch_size, encoder_output, decoder_hidden, y=None):\n",
    "        decoder_input = V(torch.zeros(batch_size).long())  \n",
    "        output_seq_stack, att_stack = [], []\n",
    "        \n",
    "        for i in range(self.max_tgt_len):\n",
    "            decoder_input = self.decoder_dropout(self.decoder_embedding_layer(decoder_input))\n",
    "            if self.attention_type == 'Bahdanau':\n",
    "                if isinstance(decoder_hidden, tuple):\n",
    "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
    "                else:\n",
    "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[-1]).unsqueeze(0) \n",
    "                att, decoder_input = self.bahdanau_attention(encoder_output, prev_hidden, decoder_input)\n",
    "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
    "                decoder_output = self.decoder_output_layer(decoder_output.squeeze(0)) \n",
    "            else:\n",
    "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
    "                decoder_output = self.decoder_output_layer(decoder_output) \n",
    "                att, decoder_output = self.luong_attention(encoder_output, decoder_output)\n",
    "            att_stack.append(att)\n",
    "            output = self.output_layer(decoder_output)\n",
    "            output_seq_stack.append(output)\n",
    "            decoder_input = V(output.data.max(1)[1])\n",
    "            if (decoder_input==1).all(): break \n",
    "            if self.teacher_forcing:    \n",
    "                samp_prob = round(random.random(),1)\n",
    "                if (y is not None) and (samp_prob < self.force_prob):\n",
    "                    if i >= len(y): break\n",
    "                    decoder_input = y[i] \n",
    "                \n",
    "        return torch.stack(output_seq_stack), torch.stack(att_stack)\n",
    "        \n",
    "                \n",
    "    def forward(self, seq, y=None):\n",
    "        batch_size = seq[0].size(0)\n",
    "        encoder_hidden = self.init_hidden(batch_size)\n",
    "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))\n",
    "        encoder_output, encoder_hidden = self.encoder_rnn(encoder_input, encoder_hidden) \n",
    "        if self.bidirectional:\n",
    "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
    "        output = self.decoder_forward(batch_size, encoder_output, encoder_hidden, y=y)\n",
    "        if isinstance(encoder_hidden, tuple):\n",
    "            encoder_vector = self.encoder_vector_layer(encoder_hidden[0][-1])\n",
    "        else:\n",
    "            encoder_vector = self.encoder_vector_layer(encoder_hidden[-1])\n",
    "        output = output + (encoder_vector,)  \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set maximum target summary size \n",
    "its = [next(model_data.trn_dl.__iter__())[1] for i in range(10)]\n",
    "max_tgt_len = int(np.percentile([its[o].size()[0] for o in range(len(its))], 99))\n",
    "max_tgt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev=0\n",
    "rev += 1\n",
    "print(\"rev = %s\" %rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, filename, file):\n",
    "    \"\"\"Function to save file as pickle\"\"\"\n",
    "    with open(f'{path}/{filename}', 'wb') as f:\n",
    "        pickle.dump(file, f)\n",
    "\n",
    "        \n",
    "def norm_pre_trained_embeddings(vecs, itos, em_sz, padding_idx):\n",
    "        \"\"\"Function to load and normalize pretrained vectors\"\"\"\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=padding_idx)\n",
    "    wgts = emb.weight.data\n",
    "    for i,w in enumerate(itos):\n",
    "        try: \n",
    "            wgts[i] = torch.from_numpy(vecs[w]-vec_mean)\n",
    "            wgts[i] = torch.from_numpy(vecs[w]/vec_std)\n",
    "        except: pass \n",
    "    emb.weight.requires_grad = False    \n",
    "    return emb\n",
    "\n",
    "\n",
    "def embedding_param(path, data_field, pre_trained_vector_type, rev, embz_size=128):\n",
    "    \"\"\"Returns embedding parameters\"\"\"\n",
    "    pre_trained=None\n",
    "    index_to_string = data_field.vocab.itos\n",
    "    string_to_index = data_field.vocab.stoi\n",
    "    vocab_path = os.path.join(path, \"vocab\")\n",
    "    os.makedirs(vocab_path, exist_ok=True)\n",
    "    save_pickle(vocab_path, 'itos_rev_{rev}.pk', index_to_string) \n",
    "    save_pickle(vocab_path, 'stoi_rev_{rev}.pk', string_to_index) \n",
    "    padding_idx = data_field.vocab.stoi['<pad>']\n",
    "    if pre_trained_vector_type:\n",
    "        vec_mean, vec_std = data_field.vocab.vectors.numpy().mean(), data_field.vocab.vectors.numpy().std()\n",
    "        print('pre_trained_vector_mean = %s, pre_trained_vector_std = %s'%(vec_mean, vec_std))\n",
    "        vector_weight_matrix = data_field.vocab.vectors\n",
    "        embz_size = vector_weight_matrix.size(1)\n",
    "        pre_trained = norm_pre_trained_embeddings(vector_weight_matrix, index_to_string, embz_size, padding_idx)\n",
    "        print('Normalizing.... \\npre_trained_vector_mean = %s, pre_trained_vector_std = %s' %(pre_trained.weight.data.numpy().mean(), pre_trained.weight.data.numpy().std()))\n",
    "    return pre_trained, embz_size, padding_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "hidden_size = 400\n",
    "output_size =  len(TEXT.vocab)\n",
    "rnn_type = 'gru'\n",
    "attention_type='bahdanau'\n",
    "tied_weight_type ='three_way'\n",
    "max_tgt_len = max_tgt_len\n",
    "pre_trained_vector,  embz_size, padding_idx = embedding_param(DATA_PATH, TEXT, pre_trained_vector_type, rev)\n",
    "model = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
    "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
    "\n",
    "print('='*100)\n",
    "print('Model log:')\n",
    "print(model, '\\n')\n",
    "print('- attention_type = {} \\n'.format(model.attention_type))\n",
    "print('- weight_tie = {} \\n'.format(model.tied_weight_type))\n",
    "print('- teacher_forcing = {} \\n '.format(model.teacher_forcing)) \n",
    "print('- pre_trained_embedding = {} \\n'.format(model.pre_trained_vector_type)) \n",
    "print('='*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.nlp import *\n",
    "from fastai.model import Stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        output = self.m(*xs, y)\n",
    "        xtra = []\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data\n",
    "    \n",
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn = RNN_Learner(model_data, SingleModel(model), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044a1ebec7c244b4b8d1129631ecbcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1457/2500 [05:42<04:05,  4.25it/s, loss=27]  "
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callbacks\n",
    "from tensorboardX import SummaryWriter\n",
    "from fastai.sgdr import Callback, DecayScheduler\n",
    "from fastai.learner import Learner\n",
    "\n",
    "class TensorboardLogger(Callback):\n",
    "    def __init__(self, path, log_name, metrics_names=[]):\n",
    "        super().__init__()\n",
    "        self.metrics_names = [\"validation_loss\"]\n",
    "        self.metrics_names += metrics_names\n",
    "        log_path = os.path.join(path, \"logs\")\n",
    "        self.log_dir = os.path.join(log_path, log_name)\n",
    "        if os.path.exists(self.log_dir): shutil.rmtree(self.log_dir)\n",
    "        os.makedirs(self.log_dir)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.iteration = 0\n",
    "        self.epoch = 0\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics):\n",
    "        self.epoch += 1\n",
    "        for val, name in zip(metrics, self.metrics_names):\n",
    "            self.writer.add_scalar(name, val, self.iteration) \n",
    "                        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):\n",
    "        self.iteration += 1\n",
    "        self.writer.add_scalar(\"training_loss\", loss, self.iteration)\n",
    "    def on_train_end(self):\n",
    "        self.writer.close()\n",
    "        \n",
    "        \n",
    "class BestModelCheckPoint(Callback):\n",
    "    def __init__(self, learner, path, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.learner = learner\n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = lr\n",
    "        self.model_log = {}\n",
    "        self.model_path = self.learner.models_path\n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "\n",
    "    def on_train_begin(self): \n",
    "        self.first_epoch = True\n",
    "        self.epoch = 0\n",
    "        self.best_loss = 0.\n",
    "\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics): \n",
    "        self.epoch += 1\n",
    "        self.val_loss = metrics[0]\n",
    "        if self.first_epoch:\n",
    "            self.best_loss = self.val_loss\n",
    "            self.first_epoch = False\n",
    "        elif self.val_loss < self.best_loss:\n",
    "            self.best_loss = self.val_loss\n",
    "            self.learner.save(self.model_name)\n",
    "            self.model_log['training_loss'] = [str(self.train_losses)]\n",
    "            self.model_log['validation_loss'] = [str(self.val_loss)]\n",
    "            self.model_log['epoch_num'] = [self.epoch]\n",
    "            self.model_log['learning_rate'] = [self.learning_rate]\n",
    "            self.model_log['model_info'] = [w for s in [str(self.learner.model)] for w in s.split('\\n')]\n",
    "            self.model_log['model_info'].append(\"(attention_type): %s\" %self.learner.model.attention_type)\n",
    "            self.model_log['model_info'].append(\"(weight_tie): %s\" %self.learner.model.tied_weight_type)\n",
    "            self.model_log['model_info'].append(\"(pre_trained_vector_type): %s\" %self.learner.model.pre_trained_vector_type)\n",
    "            self.model_log['model_info'].append(\"(teacher_forcing): %s\" %self.learner.model.teacher_forcing)\n",
    "            if self.learner.model.teacher_forcing: self.model_log['model_info'].append(\"(teacher_forcing_prob): %s\" %self.learner.model.force_prob)\n",
    "            with open(f'{self.model_path}/{self.model_name}_model_log.json', 'w') as d: json.dump(self.model_log, d)\n",
    "        else: pass        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):\n",
    "        self.train_losses = loss\n",
    "    def on_train_end(self): pass\n",
    "        \n",
    "\n",
    "class TeacherForcingSched(Callback):\n",
    "    def __init__(self, learner, scheduler):\n",
    "        super().__init__()\n",
    "        self.learner = learner\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def on_train_begin(self): \n",
    "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
    "        \n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics): \n",
    "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
    "        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):pass\n",
    "    def on_train_end(self): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345f475cd22c4f21a3292dbdecabed76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                           \n",
      "    0      2.355713   5.541737  \n",
      "    1      2.546948   5.145187                                                           \n",
      "    2      2.654558   4.7494                                                             \n",
      "    3      2.665962   4.443033                                                           \n",
      "    4      2.717336   4.327981                                                           \n",
      "    5      2.817562   4.035469                                                           \n",
      "    6      2.968624   4.194978                                                           \n",
      "    7      2.948696   4.159783                                                           \n",
      "    8      3.380821   4.035524                                                           \n",
      "    9      3.666284   4.078693                                                           \n",
      "    10     3.895586   4.208393                                                           \n",
      "    11     3.755986   4.244361                                                           \n"
     ]
    }
   ],
   "source": [
    "lr=1e-3\n",
    "model_name = f'{model.rnn_type}_{model.attention_type}_rev_{rev}'.lower()\n",
    "cycle_len=15\n",
    "best_model = BestModelCheckPoint(learn, model_data.path, model_name, lr)\n",
    "tb_logger = TensorboardLogger(model_data.path, model_name)\n",
    "sched = DecayScheduler(DecayType.LINEAR, cycle_len, 0.5, 0.0)\n",
    "teach_forcer = TeacherForcingSched(learn, sched)\n",
    "learn.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n",
    "          callbacks=[tb_logger, teach_forcer, best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def generate(x, y, m):\n",
    "    probs = m.model(V(x))\n",
    "    preds, attention, encoder_embedding = to_np(probs[0].max(2)[1]), to_np(probs[1].squeeze(1)), to_np(probs[2])\n",
    "    sentence = ' '.join([index_to_string[o] for o in x[:,0].data.cpu().numpy() if o != 1])\n",
    "    result = ' '.join([index_to_string[o] for o in preds[:,0] if o!=1])\n",
    "    orig = ' '.join([index_to_string[o] for o in y[:,0].data.cpu().numpy() if o != 1])\n",
    "    print('Input: {}'.format(sentence), '\\n')\n",
    "    print('Original summary: {}'.format(orig), '\\n')\n",
    "    print('Predicted summary: {}'.format(result))\n",
    "    attention_plot = attention[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    return preds, attention, encoder_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: french share prices fell sharply in initial trading on monday . _eos_ \n",
      "\n",
      "Original summary: french shares fall sharply _eos_ \n",
      "\n",
      "Predicted summary: french shares fall sharply . #\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = next(iter(model_data.val_dl))\n",
    "for i in range(63):\n",
    "    print(i)\n",
    "    preds, attention, encoder_embedding = generate(x.transpose(1,0)[i].unsqueeze(1), y.transpose(1,0)[i].unsqueeze(1), learn)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
